import requests
from bs4 import BeautifulSoup
import csv
import re
import os
import sys
import time

# --- ì¶”ì²œ í•„í„°ë§ í‚¤ì›Œë“œ (ì—„ë§ˆ ì¶”ì²œ: ê±´ìš° ê´€ì‹¬ì‚¬ ë° êµìœ¡/ìƒí™œìš©í’ˆ) ---
# ê¹€ê±´ìš° êµ°ì´ ì¢‹ì•„í•˜ëŠ” ê³¤ì¶©, ìƒë¬¼ í‚¤ì›Œë“œì™€ í•™ìƒìš© í‚¤ì›Œë“œë¥¼ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.
RECOMMENDED_KEYWORDS = [
    'ìœ ì¹˜ì›', 'ì´ˆë“±í•™êµ', 'ì¤‘í•™êµ', 'ê³ ë“±í•™ìƒ', 'ì…í•™', 'ì‹ í•™ê¸°', 'ì–´ë¦°ì´ë‚ ',
    'ì¥ë‚œê°', 'êµêµ¬', 'í•™ìš©í’ˆ', 'í•„ê¸°êµ¬', 'ë°±íŒ©', 'ì±…ê°€ë°©',
    'ë¬¸ì œì§‘', 'ì°¸ê³ ì„œ', 'ìŠ¤í„°ë””í”Œë˜ë„ˆ', 'ë…ì„œì‹¤', 'ìˆ˜í—˜ìƒ',
    'íƒœë¸”ë¦¿', 'ì•„ì´íŒ¨ë“œ', 'ê°¤ëŸ­ì‹œíƒ­', 'ì¸ê°•ìš©', 'ë…¸íŠ¸ë¶',
    'ìš´ë™í™”', 'í›„ë“œí‹°', 'íŒ¨ë”©', 'íŠ¸ë ˆì´ë‹ë³µ', 'ì¡°ê±°íŒ¬ì¸ ',
    'ë³´ë“œê²Œì„', 'ìŠ¬ë¼ì„', 'ë‹Œí…ë„', 'ë ˆê³ ', 'í”¼ê·œì–´', 'ë„ê°',
    'ê³¤ì¶©', 'ìƒë¬¼', 'ì‚¬ìŠ´ë²Œë ˆ', 'ì¥ìˆ˜í’ë…ì´', 'ê´€ì°°í‚¤íŠ¸', 'ìì—°ê´€ì°°'
]

def get_platform_color(platform):
    """í”Œë«í¼ ì´ë¦„ì— ë”°ë¼ í…Œë§ˆ ìƒ‰ìƒì„ ê²°ì •í•©ë‹ˆë‹¤."""
    p = platform.lower()
    if 'ì¿ íŒ¡' in p: return 'red'
    if 'ë„¤ì´ë²„' in p or 'nì‡¼í•‘' in p: return 'green'
    if '11ë²ˆê°€' in p or 'gë§ˆì¼“' in p or 'ì§€ë§ˆì¼“' in p or 'ì˜¥ì…˜' in p: return 'red'
    if 'í‹°ëª¬' in p or 'ìœ„ë©”í”„' in p: return 'blue'
    return 'blue'

def extract_price(title):
    """ì œëª© ë¬¸ìì—´ì—ì„œ ê°€ê²© ìˆ«ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ìˆ«ìì™€ ì½¤ë§ˆ ì¡°í•© ë’¤ì— 'ì›'ì´ ë¶™ëŠ” íŒ¨í„´ íƒìƒ‰
    match = re.search(r'([\d,]+)ì›', title)
    if match:
        price_str = match.group(1).replace(',', '')
        return int(price_str) if price_str.isdigit() else 0
    return 0

def collect_from_ppomppu():
    """ë½ë¿Œ í•«ë”œ ê²Œì‹œíŒ ìˆ˜ì§‘ (ë³´ì•ˆ ìš°íšŒ ë° ë‹¤ì¤‘ ì…€ë ‰í„° ì ìš©)"""
    url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu"
    
    session = requests.Session()
    # ìµœì‹  ë¸Œë¼ìš°ì € í—¤ë”ë¥¼ ë” ì •êµí•˜ê²Œ ëª¨ì‚¬í•˜ì—¬ ì°¨ë‹¨ ë°©ì§€
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://www.google.com/',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Sec-Ch-Ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"Windows"',
    }
    
    try:
        print(f"ğŸŒ ë½ë¿Œ ì„œë²„ ì ‘ì† ì‹œë„ ì¤‘: {url}")
        response = session.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # ë½ë¿Œ íŠ¹ìœ ì˜ í•œê¸€ ê¹¨ì§ ë°©ì§€ (euc-kr ëŒ€ì‘ ë° ìë™ ê°ì§€)
        if response.encoding.lower() == 'iso-8859-1':
            response.encoding = 'euc-kr'
        else:
            response.encoding = response.apparent_encoding
        
        print(f"âœ… ì ‘ì† ì„±ê³µ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
        
        # ë³´ì•ˆ í˜ì´ì§€ ë˜ëŠ” ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸
        html_content = response.text
        if "ì‚¬ìš©ìì˜ ì ‘ì†ì´ ì œí•œ" in html_content or "Robot" in html_content or "ìë™ì ‘ì†" in html_content:
            print("âŒ ì°¨ë‹¨ë¨: GitHub Actions IPê°€ ë½ë¿Œì˜ ë³´ì•ˆ ì‹œìŠ¤í…œì— ì˜í•´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"DEBUG (ì‘ë‹µ ë‚´ìš© ì¼ë¶€): {html_content[:500]}")
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
    except Exception as e:
        print(f"âŒ ì ‘ì† ë‹¨ê³„ ì¹˜ëª…ì  ì‹¤íŒ¨: {e}")
        return []
    
    collected_data = []
    
    # ë½ë¿Œì˜ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ëŠ” ë³´í†µ 'list0', 'list1' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ tr íƒœê·¸ë“¤ì…ë‹ˆë‹¤.
    rows = soup.select('tr.list0, tr.list1')
    
    if not rows:
        print("âš ï¸ í´ë˜ìŠ¤ ê¸°ë°˜ í–‰ ì°¾ê¸° ì‹¤íŒ¨. id='main_list' ê¸°ë°˜ ë°±ì—… ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        table = soup.find('table', id='main_list')
        if table:
            rows = table.find_all('tr', recursive=False)
            # í—¤ë”ë‚˜ ì˜ë¯¸ ì—†ëŠ” í–‰ ì œì™¸ë¥¼ ìœ„í•´ í•„í„°ë§
            rows = [r for r in rows if r.find('td', class_='eng v_middle')]
    
    print(f"ğŸ” ì´ {len(rows)}ê°œì˜ ì ì¬ì  í–‰(Row)ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

    for idx, row in enumerate(rows):
        try:
            # 1. ì œëª© ë° ë§í¬ íƒœê·¸ ì°¾ê¸°
            # í´ë˜ìŠ¤ list_titleì„ ìš°ì„  íƒìƒ‰í•˜ê³  ì—†ìœ¼ë©´ êµ¬ì¡°ì ìœ¼ë¡œ íƒìƒ‰
            title_tag = row.find(['font', 'span'], class_='list_title')
            if not title_tag:
                # ë½ë¿Œ êµ¬ì¡°ê°€ ë°”ë€” ê²½ìš° td ë‚´ì˜ a íƒœê·¸ë¥¼ ì§ì ‘ ì°¾ìŒ
                td_list = row.find_all('td', recursive=False)
                if len(td_list) >= 3:
                    title_tag = td_list[2].find('a')
            
            if not title_tag:
                continue

            full_title = title_tag.get_text(strip=True)
            if not full_title or len(full_title) < 5:
                continue

            # ë§í¬ ì¶”ì¶œ
            link_tag = title_tag if title_tag.name == 'a' else title_tag.find_parent('a')
            if not link_tag or not link_tag.get('href'):
                continue
            
            # ìƒì„¸ ë§í¬ ì™„ì„±
            href = link_tag['href']
            link = "https://www.ppomppu.co.kr/zboard/" + href if not href.startswith('http') else href

            # 2. ê³µì§€ì‚¬í•­ ë° ê´‘ê³ ê¸€ ì œì™¸ ë¡œì§
            # ê¸€ ë²ˆí˜¸ê°€ td.eng.v_middleì— ìˆ«ìë¡œ ë“¤ì–´ìˆëŠ”ì§€ í™•ì¸
            num_td = row.find('td', class_='eng v_middle')
            if num_td:
                num_text = num_td.get_text(strip=True)
                # ì´ë¯¸ì§€ê°€ ìˆê±°ë‚˜(ê³µì§€ ì•„ì´ì½˜), ìˆ«ìê°€ ì•„ë‹ˆë©´ ì œì™¸
                if num_td.find('img') or not num_text.isdigit():
                    continue
            else:
                # ë²ˆí˜¸ ì˜ì—­ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ê²Œì‹œê¸€ì´ ì•„ë‹ í™•ë¥ ì´ ë†’ìŒ
                continue

            # 3. í”Œë«í¼ ë° ìƒí’ˆëª…/ê°€ê²© ì •ì œ
            platform = "ê¸°íƒ€"
            p_match = re.search(r'\[(.*?)\]', full_title)
            if p_match:
                platform = p_match.group(1)
            
            price = extract_price(full_title)
            # ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ ë¶ˆí•„ìš”í•œ ëŒ€ê´„í˜¸/ê´„í˜¸ ì œê±°
            product_name = re.sub(r'\[.*?\]', '', full_title).strip()
            product_name = re.sub(r'\(.*?\)', '', product_name).strip()
            
            # --- ë±ƒì§€ ë¡œì§ (ì—„ë§ˆ ì¶”ì²œ ì ìš©) ---
            badge = "NEW"
            if any(keyword in product_name for keyword in RECOMMENDED_KEYWORDS):
                badge = "ì—„ë§ˆ ì¶”ì²œ"
            elif price > 100000:
                badge = "HOT"
            
            # 4. ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì •ê·œí™”
            img_tag = row.find('img', class_='thumb_border')
            img_url = ""
            if img_tag and img_tag.get('src'):
                src = img_tag.get('src')
                if src.startswith('//'):
                    img_url = "https:" + src
                elif src.startswith('/'):
                    img_url = "https://www.ppomppu.co.kr" + src
                else:
                    img_url = src
            else:
                img_url = f"https://placehold.co/80x80/f1f5f9/94a3b8?text={platform[:1]}"

            collected_data.append({
                "category": "í•«ë”œ",
                "platform": platform,
                "productName": product_name,
                "currentPrice": str(price),
                "originalPrice": str(int(price * 1.3)) if price > 0 else "0",
                "badge": badge,
                "sourceSite": "ë½ë¿Œ",
                "link": link,
                "image": img_url,
                "color": get_platform_color(platform)
            })
            
            if len(collected_data) >= 25: break
            
        except Exception as e:
            # ê°œë³„ í–‰ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
            print(f"âš ï¸ {idx}ë²ˆ í–‰ íŒŒì‹± ì¤‘ ê±´ë„ˆëœ€: {e}")
            continue
            
    return collected_data

def save_to_csv(data):
    """ìˆ˜ì§‘ ë°ì´í„°ë¥¼ deals.csvë¡œ ì €ì¥"""
    keys = ["category", "platform", "productName", "currentPrice", "originalPrice", "badge", "sourceSite", "link", "image", "color"]
    try:
        if not data:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤. ì €ì¥ì„ ì·¨ì†Œí•˜ê³  ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.")
            sys.exit(1)

        with open('deals.csv', 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(data)
        print(f"âœ… ìµœì¢… ì—…ë°ì´íŠ¸ ì„±ê³µ: {len(data)}ê°œì˜ í•­ëª©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    print("ğŸš€ [ë””ë²„ê·¸/ë³´ê°• ëª¨ë“œ] í•«ë”œ ìˆ˜ì§‘ ì—”ì§„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    start_time = time.time()
    
    deals = collect_from_ppomppu()
    
    if deals:
        save_to_csv(deals)
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ ì°¨ë‹¨ ì—¬ë¶€ë‚˜ êµ¬ì¡°ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.")
        sys.exit(1)
        
    end_time = time.time()
    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
